#!/usr/bin/env python3
"""
üî• SCRIPT DE PRUEBA OLLAMA PREMIUM
Prueba la conexi√≥n con los modelos premium instalados
"""

import os
import requests
import time

def load_env():
    """Cargar variables del .env"""
    try:
        with open('.env', 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        for line in lines:
            if line.startswith('OLLAMA_'):
                key, value = line.strip().split('=', 1)
                value = value.split('#')[0].strip()  # Remover comentarios
                os.environ[key] = value
                print(f'‚úÖ {key} = {value}')
    except Exception as e:
        print(f'‚ö†Ô∏è Error cargando .env: {e}')

def test_ollama_model(model_name, prompt, max_tokens=50):
    """Probar un modelo espec√≠fico"""
    url = os.environ.get('OLLAMA_URL', 'http://localhost:11434/api/generate')
    timeout = int(os.environ.get('OLLAMA_TIMEOUT', '120'))
    
    print(f'\nü§ñ Probando modelo: {model_name}')
    print(f'üîó URL: {url}')
    print(f'üí¨ Prompt: {prompt}')
    print(f'‚è±Ô∏è Timeout: {timeout}s')
    print('‚åõ Generando respuesta...')
    
    start_time = time.time()
    
    try:
        response = requests.post(url, 
            json={
                'model': model_name, 
                'prompt': prompt, 
                'stream': False, 
                'options': {
                    'num_predict': max_tokens,
                    'temperature': 0.7
                }
            }, 
            timeout=timeout)
        
        elapsed = time.time() - start_time
        print(f'üìä Status: {response.status_code} | ‚è±Ô∏è Tiempo: {elapsed:.1f}s')
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get('response', 'Sin respuesta').strip()
            print(f'‚úÖ √âXITO!')
            print(f'üìù Respuesta: {response_text}')
            return True
        else:
            print(f'‚ùå Error HTTP: {response.status_code}')
            print(f'üìÑ Detalles: {response.text[:300]}')
            return False
            
    except requests.exceptions.Timeout:
        elapsed = time.time() - start_time
        print(f'‚è±Ô∏è Timeout despu√©s de {elapsed:.1f}s - El modelo puede estar procesando')
        return False
    except Exception as e:
        elapsed = time.time() - start_time
        print(f'‚ùå Error despu√©s de {elapsed:.1f}s: {e}')
        return False

def main():
    print('üî• PRUEBA DE MODELOS OLLAMA PREMIUM')
    print('=' * 50)
    
    # Cargar configuraci√≥n
    load_env()
    
    # Modelos a probar (del m√°s r√°pido al m√°s lento)
    tests = [
        ('llama3:70b', 'Di hola en espa√±ol chileno'),
        ('mixtral:8x7b', 'Traduce al espa√±ol: "Wireless headphones"'),  
        ('codellama:34b', 'Lista 3 especificaciones t√©cnicas para auriculares')
    ]
    
    results = {}
    
    for model, prompt in tests:
        success = test_ollama_model(model, prompt, max_tokens=30)
        results[model] = success
        
        if not success:
            print('‚ö†Ô∏è Esperando 5 segundos antes del siguiente modelo...')
            time.sleep(5)
    
    # Resumen final
    print('\nüéØ RESUMEN DE RESULTADOS:')
    print('=' * 30)
    
    for model, success in results.items():
        status = '‚úÖ FUNCIONANDO' if success else '‚ùå ERROR'
        print(f'{model:<15} | {status}')
    
    total_success = sum(results.values())
    total_models = len(results)
    
    print(f'\nüìä TOTAL: {total_success}/{total_models} modelos funcionando')
    
    if total_success > 0:
        print('\nüéâ ¬°AL MENOS UN MODELO PREMIUM EST√Å FUNCIONANDO!')
        print('üöÄ Puedes proceder con el enriquecimiento de productos')
    else:
        print('\n‚ö†Ô∏è Ning√∫n modelo funciona actualmente')
        print('üí° Verifica que Ollama est√© corriendo en puerto 11434')

if __name__ == '__main__':
    main()