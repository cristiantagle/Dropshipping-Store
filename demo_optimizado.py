#!/usr/bin/env python3
"""
üî• DEMO OPTIMIZADO PARA LLAMA 3 70B
Versi√≥n corregida con timeout adecuado y prompts optimizados
"""

import os
import requests
import json
import time

def load_env():
    """Cargar variables del .env"""
    env_vars = {}
    try:
        with open('.env', 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        for line in lines:
            if line.startswith('OLLAMA_'):
                key, value = line.strip().split('=', 1)
                value = value.split('#')[0].strip()
                os.environ[key] = value
                env_vars[key] = value
                
        print("‚úÖ Variables cargadas:")
        for key, value in env_vars.items():
            print(f"   {key} = {value}")
        print()
        
    except Exception as e:
        print(f'‚ö†Ô∏è Error cargando .env: {e}')
    
    return env_vars

def call_ollama_optimized(model, prompt, max_tokens=200, show_progress=True):
    """Llamar a Ollama con configuraci√≥n optimizada"""
    url = os.environ.get('OLLAMA_URL', 'http://localhost:11434/api/generate')
    timeout = int(os.environ.get('OLLAMA_TIMEOUT', '300'))  # Usar 300s por defecto
    
    if show_progress:
        print(f"üîó URL: {url}")
        print(f"‚è±Ô∏è Timeout configurado: {timeout}s")
        print(f"üéØ Tokens m√°ximos: {max_tokens}")
    
    start_time = time.time()
    
    try:
        response = requests.post(url, 
            json={
                'model': model,
                'prompt': prompt,
                'stream': False,
                'options': {
                    'num_predict': max_tokens,
                    'temperature': 0.7,
                    'top_p': 0.9,
                    'repeat_penalty': 1.1
                }
            },
            timeout=timeout)
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get('response', '').strip()
            
            if show_progress:
                print(f"‚úÖ Respuesta recibida en {elapsed:.1f}s")
                
            return response_text
        else:
            return f"‚ùå Error HTTP {response.status_code}: {response.text[:100]}"
            
    except requests.exceptions.Timeout:
        elapsed = time.time() - start_time
        return f"‚è±Ô∏è TIMEOUT despu√©s de {elapsed:.1f}s (l√≠mite: {timeout}s)"
    except Exception as e:
        elapsed = time.time() - start_time
        return f"‚ùå Error despu√©s de {elapsed:.1f}s: {e}"

def test_single_model(model_name, test_prompt="Di 'hola' en espa√±ol chileno"):
    """Probar un modelo espec√≠fico r√°pidamente"""
    print(f"\nüß™ PRUEBA R√ÅPIDA: {model_name}")
    print("=" * 50)
    
    result = call_ollama_optimized(model_name, test_prompt, max_tokens=30)
    print(f"üìù Resultado: {result}")
    
    return not result.startswith("‚ùå") and not result.startswith("‚è±Ô∏è")

def create_optimized_description(spanish_name, category):
    """Crear descripci√≥n con prompt optimizado para Llama 3 70B"""
    
    print(f"\nü¶ô DESCRIPCI√ìN PREMIUM (Llama 3 70B)")
    print("‚åõ Generando con prompt optimizado...")
    
    # Prompt m√°s directo y espec√≠fico
    optimized_prompt = f"""Crea una descripci√≥n comercial de {spanish_name} para tienda online chilena.

Requisitos:
- 100-120 palabras exactas
- Menciona 3 beneficios principales  
- Incluye casos de uso
- Espa√±ol chileno natural
- Tono persuasivo pero profesional

Producto: {spanish_name}
Categor√≠a: {category}

Descripci√≥n:"""

    result = call_ollama_optimized('llama3:70b', optimized_prompt, max_tokens=150, show_progress=True)
    
    print(f"üìù Resultado:")
    print(f"{result}")
    
    return result

def demo_step_by_step():
    """Demo paso a paso optimizado"""
    print("üî• DEMO PASO A PASO OPTIMIZADO")
    print("=" * 60)
    
    env_vars = load_env()
    
    # Verificar configuraci√≥n
    timeout_configured = env_vars.get('OLLAMA_TIMEOUT', '120')
    print(f"üîß Timeout configurado: {timeout_configured}s")
    
    if int(timeout_configured) < 300:
        print("‚ö†Ô∏è ADVERTENCIA: Timeout puede ser insuficiente para Llama 3 70B")
        print("üí° Recomendaci√≥n: OLLAMA_TIMEOUT=300 en .env")
    
    # Productos de prueba m√°s simples
    test_products = [
        ("Wireless Headphones", "tecnologia"),
        ("Water Bottle", "hogar")
    ]
    
    for product_name, category in test_products:
        print(f"\n{'='*60}")
        print(f"üì¶ PROCESANDO: {product_name}")
        print(f"{'='*60}")
        
        # PASO 1: Traducci√≥n (r√°pida)
        print(f"\nüéØ PASO 1: Traducci√≥n con Mixtral 8x7B")
        translate_prompt = f'Traduce al espa√±ol chileno comercial: "{product_name}". Solo responde la traducci√≥n.'
        
        spanish_name = call_ollama_optimized('mixtral:8x7b', translate_prompt, 30, show_progress=False)
        print(f"‚úÖ Traducci√≥n: {spanish_name}")
        
        if spanish_name.startswith("‚ùå") or spanish_name.startswith("‚è±Ô∏è"):
            print("‚ö†Ô∏è Saltando al siguiente producto debido a error en traducci√≥n")
            continue
            
        # PASO 2: Descripci√≥n (la problem√°tica)
        description = create_optimized_description(spanish_name, category)
        
        if description.startswith("‚ùå") or description.startswith("‚è±Ô∏è"):
            print("‚ö†Ô∏è Error en descripci√≥n, probando con modelo m√°s r√°pido...")
            
            # Fallback a Mixtral para descripci√≥n
            print(f"\nüîÑ FALLBACK: Descripci√≥n con Mixtral 8x7b")
            fallback_prompt = f'Crea descripci√≥n de 80 palabras para "{spanish_name}" en espa√±ol chileno comercial.'
            fallback_desc = call_ollama_optimized('mixtral:8x7b', fallback_prompt, 100, show_progress=False)
            print(f"üìù Descripci√≥n (Mixtral): {fallback_desc}")
        
        # PASO 3: Specs (r√°pidas)
        print(f"\nüíª PASO 3: Especificaciones con CodeLlama 34B")
        specs_prompt = f'JSON de especificaciones para "{spanish_name}": material, dimensiones, peso, colores. Solo JSON:'
        
        specs = call_ollama_optimized('codellama:34b', specs_prompt, 100, show_progress=False)
        print(f"‚úÖ Especificaciones: {specs[:100]}...")
        
        print(f"\n{'='*60}")
        print(f"üéâ PRODUCTO COMPLETADO")
        print(f"{'='*60}")
        
        # Pausa entre productos
        time.sleep(5)

def main():
    """Funci√≥n principal"""
    print("üî• DEMO OPTIMIZADO PARA TU SISTEMA PREMIUM")
    print("Timeout corregido + Prompts optimizados para Llama 3 70B")
    print("‚îÅ" * 70)
    
    # Men√∫ de opciones
    print("\nüìã OPCIONES:")
    print("1. Prueba r√°pida de modelos")
    print("2. Demo completo paso a paso")
    print("3. Solo probar Llama 3 70B con descripci√≥n")
    
    choice = input("\nüéØ Elige opci√≥n (1-3): ").strip()
    
    if choice == "1":
        print("\nüß™ PRUEBAS R√ÅPIDAS")
        test_single_model('mixtral:8x7b', 'Hola')
        test_single_model('codellama:34b', 'Lista 2 materiales')
        test_single_model('llama3:70b', 'Di hola en espa√±ol')
        
    elif choice == "2":
        demo_step_by_step()
        
    elif choice == "3":
        load_env()
        create_optimized_description("Auriculares Bluetooth Premium", "tecnologia")
        
    else:
        print("üîÑ Ejecutando demo completo por defecto...")
        demo_step_by_step()
    
    print(f"\nüéä DEMO COMPLETADO")
    print("üí° Si Llama 3 70B sigue dando timeout, considera usar Mixtral 8x7B como alternativa")

if __name__ == '__main__':
    main()